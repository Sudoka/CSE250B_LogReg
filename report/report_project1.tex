\documentstyle[nips]{article}

\title{Logistic Regression with L$_2$ Regularization}

\author{Adrian Guthals, David Larson, Jason Yao \\
CSE 250B: Project \#1 \\
University of California, San Diego \\
}


\begin{document}

\maketitle


\begin{abstract}
The Abstract paragraph should be indented 1/2~inch (3~picas) on
both left and right-hand margins. Use 10~point type, with a
vertical spacing of 11~points. Two line spaces precede the Abstract.
The Abstract must be limited to one paragraph.
\end{abstract}


% Headings:
%   Section: all caps (e.g. FIRST)
%   Subsection: all caps (e.g. SECOND)
%   Subsubsection: initial caps (e.g. Third)



%-----------------------------------------------------------------------------
% INTRODUCTION
%-----------------------------------------------------------------------------
\section{INTRODUCTION}
\label{sec:intro}

This paper will attempt to recreate the results by \cite{t-logistic}.

Highlight what Logistic Regression, why is it used and where is it used (real-world applications). Also mention potential pitfalls of the method (e.g. overfitting).


%-----------------------------------------------------------------------------
% DESIGN OF ALGORITHMS
%-----------------------------------------------------------------------------
\section{DESIGN AND ANALYSIS OF ALGORITHMS}
\label{sec:algorithms}
Stochastic gradient descent (SGD) and Limited-memory Broyden-Fletcher-Goldfarb-Shannon (L-BFGS) were each implemented in Matlab to maximize the total conditional log likelihood of each training data set. In brief, SGD incorporated a fixed learning rate $\lambda$ to control the change in log likelihood, which was averaged over random mini-batches of size $\kappa$. To control over-fitting, logistic regression was used and change in the objective function was evaluated on a separate validation dataset containing 30\% of all training examples selected at random. Convergence was reached when change in the objective function was less than $\omega$ or the total number of epochs was greater than $\varepsilon$ (which ever came first). (Add brief overview of L-BFGS and cite MinFunc's implementation)

For each of these algorithms, the input training data is formatted as a set of $n$ examples $x_i \ldots x_n$ where each $x_j$ is a real-valued vector of $d$ features. Each $x_i$ is correlated to a binary (Bernoulli) outcome $y_i$ by a global parameter vector $\beta$ of length $d+1$. We assume this correlation follows the model below where $x_{i0}=1$ for all $i$.

\begin{equation}
    p_i = p(y_i|x_i;\beta) = \frac{1}{1+\exp-(\sum_{j=0}^{d} \beta_j\,x_{ij})}
\end{equation}


\subsection{STOCHASTIC GRADIENT DESCENT} 
Our SGD implementation first randomized the order of input examples to avoid repeated computation of random numbers and partitioned the input data into $x_1 \ldots x_k$ \emph{training} examples and $x_{k+1} \ldots x_n$ \emph{validation} examples. Then sequential mini-batches of size $\kappa < k$ taken from the training set were used to update the parameter vector $\beta$ (initialized to all zero values) by the following equation. The constant $\mu$ quantifies the trade-off between maximizing likelihood and minimizing parameter values for $L_2$ Regularization.

\begin{equation}
    \beta := \beta + \frac{\lambda}{\kappa}\,[-2 \mu \beta + \sum_{i=1}^{\kappa} (y_i - p_i)\,x_i]
\end{equation}

After each update of $\beta$, absolute change in the objective $\widehat{\beta}$ was computed over all validation examples with the following function.

\begin{equation}
    \widehat{\beta} = \mu \|\beta\|_2 + \sum_{i=k+1}^{n}{-\log(p_i^{y_i}(1 - p_i)^{1-y_i})}
\end{equation}

Convergence was reached when change in the objective reached a value less than $\omega$. Convergence was also reached if the total number of epochs was greater than $\varepsilon$. With this configuration, the time to run the SGD is O(nd).

\subsubsection{Cross Validation}
The algorithm was run ten times to generate 10 vectors of parameters. The ten vectors of parameters were averaged to calculate one vector with cross-validation. The result cross-validated vector was used to calculate the test error and its variance over all the instances. The test error was defined as the average value of the loss function:
\begin{equation}
        \textrm{test error} = \sum_{i=1}^{n} -log(y_i | x_i ; \beta )
\end{equation}



\subsection{L2 REGULARIZATION}
\begin{equation}
    \hat{B} = \textrm{argmax}_{\beta} LCL - \mu||\beta||_2^2
\end{equation}
where $||\beta||_2^2$ is the $L_2$ norm of the parameter vector.


\begin{equation}
    \frac{\partial}{\partial \beta_j} LCL = \sum_i (y_i - p_i)x_{ij}
\end{equation}


\subsection{LIMITED-MEMORY BFGS}
Limited-memory Broyden-Fletcher-Goldfarb-Shannon (L-BFGS) is a quasi-Newton
optimization method used to find local extrema.



%-----------------------------------------------------------------------------
% DESIGN OF EXPERIMENTS
%-----------------------------------------------------------------------------
\section{DESIGN OF EXPERIMENTS}
\label{sec:experiments}

Design of experiments.

\subsection{HYPERPARAMETERS}

\subsubsection{Learning Rate}
How did we determine $\lambda$ (the learning rate).

\subsubsection{Regularization}
How did we determine $\mu$ (the regularization constant).

\subsubsection{SGD Subset}
How did we determine (or why did we choose) the length of subsets used by the SGD (1 row at a time, 2 at a time, etc.)



%-----------------------------------------------------------------------------
% RESULTS OF EXPERIMENTS
%-----------------------------------------------------------------------------
\section{RESULTS OF EXPERIMENTS}
\label{sec:results}

Results of experiments. Comparison of methods and data sets.

\subsection{USPS-N}
How did SGD and L-BFGS perform on the USPS-N data sets.

\subsection{WEB}
How did SGD and L-BFGS perform on the Web data sets.


\subsubsection{Figures} 

\begin{figure}[h]
\vspace{1in}
\caption{Comparison of SGD and L-BFGS for USPS-N (left) and Web (right) data sets.}
\end{figure}

\begin{table}[t]
\caption{Hyperparameters.}
\label{tab-hyperparameters}
\begin{center}
\begin{tabular}{lll}
& $\lambda$  & $\mu$
\\ \hline \\
SGD & 1.0  & 1.0 \\
SGD & 0.1  & 1.0 \\
SGD & 0.01 & 1.0 \\
\end{tabular}
\end{center}
\end{table}



%-----------------------------------------------------------------------------
% FINDINGS AND LESSONS LEARNED
%-----------------------------------------------------------------------------
\section{FINDINGS AND LESSONS LEARNED}
\label{sec:conclusion}

Findings and lessons learned.



%-----------------------------------------------------------------------------
% BIBLIOGRAPHY
%-----------------------------------------------------------------------------
\bibliographystyle{IEEEtran}
\bibliography{sources}


\end{document}
